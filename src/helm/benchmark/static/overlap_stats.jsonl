{"models": ["together/bloom", "together/gpt-j-6b", "together/gpt-neox-20b", "together/opt-66b", "together/opt-175b", "microsoft/TNLGv2_7B", "microsoft/TNLGv2_530B", "anthropic/stanford-online-all-v4-s3", "anthropic/claude-v1.3", "anthropic/claude-instant-v1", "together/yalm"], "group_overlap_stats_list": [{"group": "boolq", "num_instances": 3334, "num_overlapping_inputs": 1769, "num_overlapping_references": 0}, {"group": "narrative_qa", "num_instances": 355, "num_overlapping_inputs": 175, "num_overlapping_references": 0}, {"group": "news_qa", "num_instances": 1273, "num_overlapping_inputs": 797, "num_overlapping_references": 34}, {"group": "quac", "num_instances": 1000, "num_overlapping_inputs": 817, "num_overlapping_references": 222}, {"group": "natural_qa_openbook_longans", "num_instances": 2144, "num_overlapping_inputs": 1010, "num_overlapping_references": 100}, {"group": "natural_qa_closedbook", "num_instances": 2144, "num_overlapping_inputs": 1, "num_overlapping_references": 100}, {"group": "commonsenseqa", "num_instances": 1221, "num_overlapping_inputs": 1, "num_overlapping_references": 0}, {"group": "hellaswag", "num_instances": 10042, "num_overlapping_inputs": 306, "num_overlapping_references": 275}, {"group": "openbookqa", "num_instances": 500, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "truthful_qa", "num_instances": 654, "num_overlapping_inputs": 1, "num_overlapping_references": 2}, {"group": "mmlu", "num_instances": 14042, "num_overlapping_inputs": 572, "num_overlapping_references": 62}, {"group": "msmarco_regular", "num_instances": 6979, "num_overlapping_inputs": 2, "num_overlapping_references": 6845}, {"group": "msmarco_trec", "num_instances": 43, "num_overlapping_inputs": 0, "num_overlapping_references": 43}, {"group": "summarization_cnndm", "num_instances": 11490, "num_overlapping_inputs": 3905, "num_overlapping_references": 323}, {"group": "summarization_xsum", "num_instances": 11334, "num_overlapping_inputs": 4133, "num_overlapping_references": 870}, {"group": "imdb", "num_instances": 25712, "num_overlapping_inputs": 546, "num_overlapping_references": 0}, {"group": "raft", "num_instances": 440, "num_overlapping_inputs": 62, "num_overlapping_references": 0}, {"group": "civil_comments", "num_instances": 317754, "num_overlapping_inputs": 10924, "num_overlapping_references": 0}, {"group": "blimp", "num_instances": 67000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "wikitext_103", "num_instances": 60, "num_overlapping_inputs": 43, "num_overlapping_references": 0}, {"group": "the_pile", "num_instances": 21455, "num_overlapping_inputs": 7009, "num_overlapping_references": 0}, {"group": "twitter_aae", "num_instances": 100000, "num_overlapping_inputs": 392, "num_overlapping_references": 0}, {"group": "twitter_aae_aa", "num_instances": 50000, "num_overlapping_inputs": 66, "num_overlapping_references": 0}, {"group": "twitter_aae_white", "num_instances": 50000, "num_overlapping_inputs": 326, "num_overlapping_references": 0}, {"group": "ice", "num_instances": 13089, "num_overlapping_inputs": 1673, "num_overlapping_references": 0}, {"group": "wikifact", "num_instances": 58426, "num_overlapping_inputs": 18, "num_overlapping_references": 441}, {"group": "synthetic_reasoning", "num_instances": 25000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "synthetic_reasoning_pattern_match", "num_instances": 5000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "synthetic_reasoning_variable_substitution", "num_instances": 5000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "synthetic_reasoning_induction", "num_instances": 5000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "synthetic_reasoning_natural", "num_instances": 10000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "babi_qa", "num_instances": 40000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "dyck_language", "num_instances": 1500, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "math_regular", "num_instances": 5000, "num_overlapping_inputs": 89, "num_overlapping_references": 2}, {"group": "math_chain_of_thought", "num_instances": 5000, "num_overlapping_inputs": 89, "num_overlapping_references": 73}, {"group": "gsm", "num_instances": 1319, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "legal_support", "num_instances": 3047, "num_overlapping_inputs": 267, "num_overlapping_references": 632}, {"group": "lsat_qa", "num_instances": 459, "num_overlapping_inputs": 10, "num_overlapping_references": 0}, {"group": "lex_glue", "num_instances": 23607, "num_overlapping_inputs": 14964, "num_overlapping_references": 0}, {"group": "legal_summarization", "num_instances": 4073, "num_overlapping_inputs": 3959, "num_overlapping_references": 1780}, {"group": "summarization", "num_instances": 4073, "num_overlapping_inputs": 3959, "num_overlapping_references": 1780}, {"group": "MedQA", "num_instances": 1273, "num_overlapping_inputs": 197, "num_overlapping_references": 1}, {"group": "entity_matching", "num_instances": 2116, "num_overlapping_inputs": 56, "num_overlapping_references": 0}, {"group": "entity_data_imputation", "num_instances": 151, "num_overlapping_inputs": 1, "num_overlapping_references": 0}, {"group": "code_humaneval", "num_instances": 164, "num_overlapping_inputs": 3, "num_overlapping_references": 4}, {"group": "code_apps", "num_instances": 5000, "num_overlapping_inputs": 1320, "num_overlapping_references": 464}, {"group": "copyright_text", "num_instances": 999, "num_overlapping_inputs": 75, "num_overlapping_references": 201}, {"group": "copyright_code", "num_instances": 5978, "num_overlapping_inputs": 6, "num_overlapping_references": 36}, {"group": "disinformation", "num_instances": 54, "num_overlapping_inputs": 2, "num_overlapping_references": 0}, {"group": "disinformation_reiteration", "num_instances": 43, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "disinformation_wedging", "num_instances": 11, "num_overlapping_inputs": 2, "num_overlapping_references": 0}, {"group": "bbq", "num_instances": 2000, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "real_toxicity_prompts", "num_instances": 99442, "num_overlapping_inputs": 6692, "num_overlapping_references": 0}, {"group": "bold", "num_instances": 14402, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "robustness_contrast_sets", "num_instances": 420, "num_overlapping_inputs": 40, "num_overlapping_references": 0}, {"group": "self_instruct", "num_instances": 252, "num_overlapping_inputs": 12, "num_overlapping_references": 19}, {"group": "grammar", "num_instances": 320, "num_overlapping_inputs": 8, "num_overlapping_references": 0}, {"group": "open_assistant", "num_instances": 188, "num_overlapping_inputs": 1, "num_overlapping_references": 31}, {"group": "vicuna", "num_instances": 80, "num_overlapping_inputs": 0, "num_overlapping_references": 0}, {"group": "koala", "num_instances": 180, "num_overlapping_inputs": 4, "num_overlapping_references": 0}, {"group": "anthropic_hh_rlhf", "num_instances": 47513, "num_overlapping_inputs": 66, "num_overlapping_references": 0}]}
